import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score
import time
from collections import defaultdict
import random

def learning_curve(model_class, x_data, y_data, x_test, y_test, text_processor,
                   train_sizes=None, cv_folds=3, model_params=None,
                   title="Learning Curve", figsize=(12, 8)):
    """
    V·∫Ω learning curve ƒë·ªÉ ki·ªÉm ƒë·ªãnh underfitting v√† overfitting

    Args:
        model_class: Class c·ªßa model (NaiveBayes ho·∫∑c NaiveBayesCUDAOptimized)
        x_data: D·ªØ li·ªáu vƒÉn b·∫£n ƒë·∫ßy ƒë·ªß
        y_data: Nh√£n ƒë·∫ßy ƒë·ªß t∆∞∆°ng ·ª©ng
        x_test: D·ªØ li·ªáu test
        y_test: Nh√£n test
        text_processor: TextProcessor ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n
        train_sizes: List k√≠ch th∆∞·ªõc t·∫≠p train (m·∫∑c ƒë·ªãnh t·ª´ 500 ƒë·∫øn to√†n b·ªô)
        cv_folds: S·ªë folds cho cross-validation
        model_params: Parameters cho model
        title: Ti√™u ƒë·ªÅ bi·ªÉu ƒë·ªì
        figsize: K√≠ch th∆∞·ªõc figure

    Returns:
        dict: K·∫øt qu·∫£ learning curve
    """

    if model_params is None:
        model_params = {'alpha': 0.001}

    # T√≠nh s·ªë l∆∞·ª£ng m·∫´u tr√™n m·ªói l·ªõp
    class_counts = defaultdict(int)
    for label in y_data:
        class_counts[label] += 1

    min_samples_per_class = min(class_counts.values())
    print(f"S·ªë m·∫´u t·ªëi thi·ªÉu tr√™n m·ªói l·ªõp: {min_samples_per_class}")
    print(f"Ph√¢n ph·ªëi l·ªõp: {dict(class_counts)}")

    # Thi·∫øt l·∫≠p train_sizes m·∫∑c ƒë·ªãnh
    if train_sizes is None:
        max_size_per_class = min(3500, min_samples_per_class)
        train_sizes = list(range(500, max_size_per_class + 1, 500))
        # Th√™m m·ªôt s·ªë k√≠ch th∆∞·ªõc nh·ªè h∆°n ƒë·ªÉ th·∫•y r√µ underfitting
        train_sizes = [100, 200] + train_sizes
        train_sizes = sorted(list(set(train_sizes)))

    print(f"K√≠ch th∆∞·ªõc train s·∫Ω test: {train_sizes}")

    # Chu·∫©n b·ªã d·ªØ li·ªáu theo l·ªõp
    class_data = defaultdict(list)
    for i, label in enumerate(y_data):
        class_data[label].append((x_data[i], label))

    # Shuffle d·ªØ li·ªáu trong m·ªói l·ªõp
    for label in class_data:
        random.shuffle(class_data[label])

    # X·ª≠ l√Ω d·ªØ li·ªáu test m·ªôt l·∫ßn
    print("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu test...")
    x_test_processed = text_processor.transform(x_test)

    # L∆∞u k·∫øt qu·∫£
    results = {
        'train_sizes': [],
        'train_scores_mean': [],
        'train_scores_std': [],
        'test_scores_mean': [],
        'test_scores_std': [],
        'fit_times_mean': [],
        'fit_times_std': []
    }

    print("\n=== B·∫ÆT ƒê·∫¶U LEARNING CURVE ===")

    for size_per_class in train_sizes:
        print(f"\n--- Training v·ªõi {size_per_class} m·∫´u/l·ªõp ---")

        # Ki·ªÉm tra xem c√≥ ƒë·ªß d·ªØ li·ªáu kh√¥ng
        if size_per_class > min_samples_per_class:
            print(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {size_per_class} m·∫´u/l·ªõp, b·ªè qua.")
            continue

        total_train_size = size_per_class * len(class_data)

        # Cross-validation
        train_scores = []
        test_scores = []
        fit_times = []

        for fold in range(cv_folds):
            print(f"  Fold {fold + 1}/{cv_folds}...")

            # T·∫°o t·∫≠p train cho fold n√†y
            fold_x_train = []
            fold_y_train = []

            for label, data_list in class_data.items():
                # Chia d·ªØ li·ªáu th√†nh cv_folds ph·∫ßn
                fold_size = len(data_list) // cv_folds
                start_idx = fold * fold_size
                end_idx = start_idx + fold_size

                # L·∫•y d·ªØ li·ªáu cho fold hi·ªán t·∫°i (ph·∫ßn c√≤n l·∫°i l√†m validation)
                fold_data = data_list[:start_idx] + data_list[end_idx:]

                # L·∫•y size_per_class m·∫´u ƒë·∫ßu ti√™n
                selected_data = fold_data[:size_per_class]

                for text, label_item in selected_data:
                    fold_x_train.append(text)
                    fold_y_train.append(label_item)

            # X·ª≠ l√Ω d·ªØ li·ªáu train cho fold n√†y
            fold_x_train_processed = text_processor.transform(fold_x_train)
            feature_names = text_processor.get_feature_names()

            # Train model
            start_time = time.time()

            try:
                # Th·ª≠ GPU model tr∆∞·ªõc
                if 'use_gpu' in model_class.__init__.__code__.co_varnames:
                    model = model_class(use_gpu=True, **model_params)
                else:
                    model = model_class(**model_params)
            except:
                model = model_class(**model_params)

            model.fit(fold_x_train_processed, fold_y_train, feature_names)
            fit_time = time.time() - start_time
            fit_times.append(fit_time)

            # ƒê√°nh gi√° tr√™n t·∫≠p train
            train_pred = model.predict(fold_x_train_processed)
            train_accuracy = accuracy_score(fold_y_train, train_pred)
            train_scores.append(train_accuracy)

            # ƒê√°nh gi√° tr√™n t·∫≠p test
            test_pred = model.predict(x_test_processed)
            test_accuracy = accuracy_score(y_test, test_pred)
            test_scores.append(test_accuracy)

            print(f"    Train accuracy: {train_accuracy:.4f}")
            print(f"    Test accuracy: {test_accuracy:.4f}")
            print(f"    Fit time: {fit_time:.2f}s")

        # T√≠nh trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n
        results['train_sizes'].append(total_train_size)
        results['train_scores_mean'].append(np.mean(train_scores))
        results['train_scores_std'].append(np.std(train_scores))
        results['test_scores_mean'].append(np.mean(test_scores))
        results['test_scores_std'].append(np.std(test_scores))
        results['fit_times_mean'].append(np.mean(fit_times))
        results['fit_times_std'].append(np.std(fit_times))

        print(f"  Trung b√¨nh - Train: {np.mean(train_scores):.4f} ¬± {np.std(train_scores):.4f}")
        print(f"  Trung b√¨nh - Test: {np.mean(test_scores):.4f} ¬± {np.std(test_scores):.4f}")

    # V·∫Ω bi·ªÉu ƒë·ªì
    plot_learning_curve(results, title, figsize)

    # Ph√¢n t√≠ch k·∫øt qu·∫£
    analyze_learning_curve(results)

    return results

def plot_learning_curve(results, title="Learning Curve", figsize=(12, 8)):
    """V·∫Ω bi·ªÉu ƒë·ªì learning curve"""

    plt.figure(figsize=figsize)

    train_sizes = results['train_sizes']
    train_scores_mean = results['train_scores_mean']
    train_scores_std = results['train_scores_std']
    test_scores_mean = results['test_scores_mean']
    test_scores_std = results['test_scores_std']

    # V·∫Ω ƒë∆∞·ªùng train score
    plt.plot(train_sizes, train_scores_mean, 'o-', color='blue',
             label='Training Accuracy', linewidth=2, markersize=8)
    plt.fill_between(train_sizes,
                     np.array(train_scores_mean) - np.array(train_scores_std),
                     np.array(train_scores_mean) + np.array(train_scores_std),
                     alpha=0.2, color='blue')

    # V·∫Ω ƒë∆∞·ªùng test score
    plt.plot(train_sizes, test_scores_mean, 'o-', color='red',
             label='Testing Accuracy', linewidth=2, markersize=8)
    plt.fill_between(train_sizes,
                     np.array(test_scores_mean) - np.array(test_scores_std),
                     np.array(test_scores_mean) + np.array(test_scores_std),
                     alpha=0.2, color='red')

    # T√πy ch·ªânh bi·ªÉu ƒë·ªì
    plt.xlabel('Training Set Size', fontsize=12)
    plt.ylabel('Accuracy Score', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=11)
    plt.grid(True, alpha=0.3)

    # Thi·∫øt l·∫≠p tr·ª•c
    plt.ylim(0, 1.05)
    plt.xlim(min(train_sizes) * 0.9, max(train_sizes) * 1.1)

    # Th√™m ch√∫ th√≠ch v·ªÅ overfitting/underfitting
    max_gap = max(np.array(train_scores_mean) - np.array(test_scores_mean))
    plt.text(0.02, 0.98, f'Max Train-Test Gap: {max_gap:.3f}',
             transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.show()

def analyze_learning_curve(results):
    """Ph√¢n t√≠ch k·∫øt qu·∫£ learning curve"""

    print("\n" + "="*60)
    print("PH√ÇN T√çCH LEARNING CURVE")
    print("="*60)

    train_scores = np.array(results['train_scores_mean'])
    test_scores = np.array(results['test_scores_mean'])
    train_sizes = np.array(results['train_sizes'])

    # 1. Ph√¢n t√≠ch Underfitting
    print("\n1. PH√ÇN T√çCH UNDERFITTING:")
    initial_train_score = train_scores[0]
    initial_test_score = test_scores[0]

    if initial_train_score < 0.7 or initial_test_score < 0.7:
        print("   ‚ö†Ô∏è  UNDERFITTING ƒë∆∞·ª£c ph√°t hi·ªán!")
        print(f"   - Train accuracy ban ƒë·∫ßu: {initial_train_score:.3f}")
        print(f"   - Test accuracy ban ƒë·∫ßu: {initial_test_score:.3f}")
        print("   üìã Khuy·∫øn ngh·ªã:")
        print("      ‚Ä¢ Gi·∫£m regularization (alpha)")
        print("      ‚Ä¢ TƒÉng s·ªë l∆∞·ª£ng features (max_features)")
        print("      ‚Ä¢ Th·ª≠ m√¥ h√¨nh ph·ª©c t·∫°p h∆°n")
    else:
        print("   ‚úÖ Kh√¥ng c√≥ d·∫•u hi·ªáu underfitting r√µ r√†ng")

    # 2. Ph√¢n t√≠ch Overfitting
    print("\n2. PH√ÇN T√çCH OVERFITTING:")
    gaps = train_scores - test_scores
    max_gap = np.max(gaps)
    avg_gap = np.mean(gaps)

    print(f"   - Gap trung b√¨nh (Train - Test): {avg_gap:.3f}")
    print(f"   - Gap l·ªõn nh·∫•t: {max_gap:.3f}")

    if max_gap > 0.1:
        print("   ‚ö†Ô∏è  OVERFITTING ƒë∆∞·ª£c ph√°t hi·ªán!")
        print("   üìã Khuy·∫øn ngh·ªã:")
        print("      ‚Ä¢ TƒÉng regularization (alpha)")
        print("      ‚Ä¢ Gi·∫£m s·ªë l∆∞·ª£ng features")
        print("      ‚Ä¢ Thu th·∫≠p th√™m d·ªØ li·ªáu training")
        print("      ‚Ä¢ S·ª≠ d·ª•ng cross-validation")
    elif max_gap > 0.05:
        print("   ‚ö†Ô∏è  C√≥ d·∫•u hi·ªáu overfitting nh·∫π")
        print("   üìã Khuy·∫øn ngh·ªã: Theo d√µi v√† c√≥ th·ªÉ tƒÉng nh·∫π regularization")
    else:
        print("   ‚úÖ Kh√¥ng c√≥ d·∫•u hi·ªáu overfitting ƒë√°ng k·ªÉ")

    # 3. Ph√¢n t√≠ch xu h∆∞·ªõng
    print("\n3. PH√ÇN T√çCH XU H∆Ø·ªöNG:")

    # Xu h∆∞·ªõng train score
    if len(train_scores) >= 3:
        train_trend = np.polyfit(range(len(train_scores)), train_scores, 1)[0]
        test_trend = np.polyfit(range(len(test_scores)), test_scores, 1)[0]

        print(f"   - Xu h∆∞·ªõng Train accuracy: {train_trend:+.6f}/step")
        print(f"   - Xu h∆∞·ªõng Test accuracy: {test_trend:+.6f}/step")

        if test_trend > 0.001:
            print("   üìà Test accuracy ƒëang tƒÉng - c√≥ th·ªÉ c·∫ßn th√™m d·ªØ li·ªáu")
        elif test_trend < -0.001:
            print("   üìâ Test accuracy ƒëang gi·∫£m - c√≥ d·∫•u hi·ªáu overfitting")
        else:
            print("   ‚û°Ô∏è  Test accuracy ·ªïn ƒë·ªãnh - m√¥ h√¨nh ƒë√£ h·ªôi t·ª•")

    # 4. Khuy·∫øn ngh·ªã t·ªïng qu√°t
    print("\n4. KHUY·∫æN NGH·ªä T·ªîNG QU√ÅT:")

    final_train_score = train_scores[-1]
    final_test_score = test_scores[-1]
    final_gap = final_train_score - final_test_score

    if final_test_score > 0.9:
        print("   üéØ Hi·ªáu su·∫•t t·ªët!")
    elif final_test_score > 0.8:
        print("   üëç Hi·ªáu su·∫•t kh√° t·ªët")
    else:
        print("   üîß C·∫ßn c·∫£i thi·ªán hi·ªáu su·∫•t")

    if final_gap < 0.03:
        print("   ‚öñÔ∏è  M√¥ h√¨nh c√¢n b·∫±ng t·ªët")
    elif final_gap < 0.1:
        print("   ‚öñÔ∏è  M√¥ h√¨nh t∆∞∆°ng ƒë·ªëi c√¢n b·∫±ng")
    else:
        print("   ‚ö†Ô∏è  M√¥ h√¨nh kh√¥ng c√¢n b·∫±ng - c·∫ßn ƒëi·ªÅu ch·ªânh")

    # 5. Th·ªëng k√™ chi ti·∫øt
    print(f"\n5. TH·ªêNG K√ä CHI TI·∫æT:")
    print(f"   - K√≠ch th∆∞·ªõc training nh·ªè nh·∫•t: {min(train_sizes):,}")
    print(f"   - K√≠ch th∆∞·ªõc training l·ªõn nh·∫•t: {max(train_sizes):,}")
    print(f"   - Train accuracy cu·ªëi: {final_train_score:.4f}")
    print(f"   - Test accuracy cu·ªëi: {final_test_score:.4f}")
    print(f"   - Th·ªùi gian fit trung b√¨nh: {np.mean(results['fit_times_mean']):.2f}s")

# H√†m s·ª≠ d·ª•ng v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø
def run_learning_curve_analysis():
    """
    Ch·∫°y ph√¢n t√≠ch learning curve v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø
    """
    from src.main.python.models.data import DataLoaderFactory
    from src.main.python.models.text_processor import TextProcessor
    from src.main.python.algorithms.classification.naive_bayes import NaiveBayes
    from src.main.python.algorithms.classification.naive_bayes_cuda_optimized import NaiveBayesCUDAOptimized

    print("=== PH√ÇN T√çCH LEARNING CURVE CHO NAIVE BAYES ===")

    # Load d·ªØ li·ªáu
    print("ƒêang t·∫£i d·ªØ li·ªáu...")
    loader = DataLoaderFactory.create_loader(
        "huggingface",
        dataset_name="papluca/language-identification",
    )
    x_train, y_train = loader.load_data(loader.dataset_name, "train")
    x_test, y_test = loader.load_data(loader.dataset_name, "test")

    # Chu·∫©n b·ªã text processor
    print("ƒêang chu·∫©n b·ªã text processor...")
    text_processor = TextProcessor(ngram_range=(1, 3), max_features=5000)
    text_processor.fit_transform(x_train)

    # Ch·∫°y learning curve cho CPU model
    print("\n" + "="*50)
    print("LEARNING CURVE CHO NAIVE BAYES (CPU)")
    print("="*50)

    cpu_results = learning_curve(
        model_class=NaiveBayes,
        x_data=x_train,
        y_data=y_train,
        x_test=x_test,
        y_test=y_test,
        text_processor=text_processor,
        train_sizes=[100, 200, 500, 1000, 1500, 2000, 2500, 3000],
        cv_folds=3,
        model_params={'alpha': 0.001},
        title="Learning Curve - Naive Bayes (CPU)",
        figsize=(12, 8)
    )

    # Ch·∫°y learning curve cho GPU model (n·∫øu c√≥)
    try:
        print("\n" + "="*50)
        print("LEARNING CURVE CHO NAIVE BAYES CUDA (GPU)")
        print("="*50)

        gpu_results = learning_curve(
            model_class=NaiveBayesCUDAOptimized,
            x_data=x_train,
            y_data=y_train,
            x_test=x_test,
            y_test=y_test,
            text_processor=text_processor,
            train_sizes=[100, 200, 500, 1000, 1500, 2000, 2500, 3000],
            cv_folds=3,
            model_params={'alpha': 0.001, 'use_gpu': True},
            title="Learning Curve - Naive Bayes CUDA (GPU)",
            figsize=(12, 8)
        )

        # So s√°nh CPU vs GPU
        compare_cpu_gpu_performance(cpu_results, gpu_results)

    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ch·∫°y GPU learning curve: {e}")

    return cpu_results

def compare_cpu_gpu_performance(cpu_results, gpu_results):
    """So s√°nh hi·ªáu su·∫•t gi·ªØa CPU v√† GPU"""

    print("\n" + "="*50)
    print("SO S√ÅNH HI·ªÜU SU·∫§T CPU vs GPU")
    print("="*50)

    # So s√°nh th·ªùi gian training
    cpu_times = np.array(cpu_results['fit_times_mean'])
    gpu_times = np.array(gpu_results['fit_times_mean'])

    print(f"Th·ªùi gian training trung b√¨nh:")
    print(f"  CPU: {np.mean(cpu_times):.2f}s ¬± {np.std(cpu_times):.2f}s")
    print(f"  GPU: {np.mean(gpu_times):.2f}s ¬± {np.std(gpu_times):.2f}s")
    print(f"  Speedup: {np.mean(cpu_times)/np.mean(gpu_times):.2f}x")

    # So s√°nh accuracy
    cpu_final_acc = cpu_results['test_scores_mean'][-1]
    gpu_final_acc = gpu_results['test_scores_mean'][-1]

    print(f"\nAccuracy cu·ªëi c√πng:")
    print(f"  CPU: {cpu_final_acc:.4f}")
    print(f"  GPU: {gpu_final_acc:.4f}")
    print(f"  Ch√™nh l·ªách: {abs(gpu_final_acc - cpu_final_acc):.4f}")

    # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
    plt.figure(figsize=(15, 5))

    # Subplot 1: So s√°nh accuracy
    plt.subplot(1, 3, 1)
    train_sizes = cpu_results['train_sizes']
    plt.plot(train_sizes, cpu_results['test_scores_mean'], 'o-', label='CPU', linewidth=2)
    plt.plot(train_sizes, gpu_results['test_scores_mean'], 's-', label='GPU', linewidth=2)
    plt.xlabel('Training Size')
    plt.ylabel('Test Accuracy')
    plt.title('Accuracy Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 2: So s√°nh th·ªùi gian training
    plt.subplot(1, 3, 2)
    plt.plot(train_sizes, cpu_times, 'o-', label='CPU', linewidth=2)
    plt.plot(train_sizes, gpu_times, 's-', label='GPU', linewidth=2)
    plt.xlabel('Training Size')
    plt.ylabel('Training Time (s)')
    plt.title('Training Time Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 3: Speedup
    plt.subplot(1, 3, 3)
    speedup = cpu_times / gpu_times
    plt.plot(train_sizes, speedup, 'g^-', linewidth=2, markersize=8)
    plt.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')
    plt.xlabel('Training Size')
    plt.ylabel('Speedup (CPU time / GPU time)')
    plt.title('GPU Speedup')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Ch·∫°y ph√¢n t√≠ch learning curve
    results = run_learning_curve_analysis()